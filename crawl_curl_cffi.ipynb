{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8gUC0RucUCOC"
      },
      "outputs": [],
      "source": [
        "\n",
        "from curl_cffi import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "from unidecode import unidecode\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "from time import sleep\n",
        "from random import randint\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import NoSuchElementException,StaleElementReferenceException\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import re\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "local_path = os.getenv('RAW_PATH')\n",
        "query_crawl = os.getenv('QUERY_CRAWL_PATH')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hàm lấy danh mục"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Danh mục parent - c1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lấy tất cả các danh mục cha - danh mục cấp 1\n",
        "def crawl_all_categories_c1 ():\n",
        "\n",
        "  url = 'https://www.fahasa.com/'\n",
        "\n",
        "  response = requests.get(url, impersonate='chrome')\n",
        "\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "  cate_books = soup.find_all('li', class_ = ['parent', 'dropdown' , 'parent dropdown aligned-left'])\n",
        "  json_categories = []\n",
        "  for cate in cate_books:\n",
        "    title = cate.find('a')\n",
        "    category = {}\n",
        "    category['Tên danh mục c1'] = title.get_text()\n",
        "    category['Liên kết'] = title.attrs['href']\n",
        "    json_categories.append(category)\n",
        "\n",
        "  df_categories = pd.json_normalize(json_categories)\n",
        "  return df_categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Danh mục child - c2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Lấy tất cả các danh mục cấp 2 - danh mục con\n",
        "def crawl_all_categories_c2(category_name_c1, category_link_c1):\n",
        "\n",
        "\n",
        "  url = category_link_c1\n",
        "\n",
        "  response = requests.get(url, impersonate='chrome')\n",
        "\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "\n",
        "  cate_books = soup.find_all('ol', id = 'children-categories')\n",
        "  json_categories = []\n",
        "  for cate in cate_books:\n",
        "    title = cate.find_all('a')\n",
        "    for i in title:\n",
        "        category = {}\n",
        "        category['Tên danh mục c1'] = category_name_c1\n",
        "        category['Tên danh mục c2'] = i.get_text()\n",
        "        category['Liên kết'] = i.attrs['href']\n",
        "        category['Mã danh mục'] = i.attrs['cat_id']\n",
        "        json_categories.append(category)\n",
        "\n",
        "  df_categories_c2 = pd.json_normalize(json_categories)\n",
        "  return df_categories_c2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Craw dữ liệu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hàm crawl comments thông qua selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crawl_comments_selenium(product_link, product_id):\n",
        "    \n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--disable-notifications\")\n",
        "    \n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "    \n",
        "    driver.get(product_link)\n",
        "    # cần refresh lại 1 lần trước khi scroll xuống do get lần đầu phần comment không laod\n",
        "    driver.refresh()\n",
        "\n",
        "    #scroll tới cuối trang để load phần comments\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "\n",
        "    # chờ 2-5s để load phần comment\n",
        "    sleep(randint(2, 5))\n",
        "\n",
        "    comments_of_product = []\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            #try catch để tránh trường hợp không có comment\n",
        "            comments = driver.find_element(By.CLASS_NAME, 'comment_list').find_elements(By.TAG_NAME, 'li')\n",
        "        except NoSuchElementException:\n",
        "            driver.refresh()\n",
        "            continue    \n",
        "        for comment in comments:\n",
        "            try:\n",
        "                    content_comment = comment.text.split('\\n')\n",
        "                    dict_comment = {}\n",
        "                    # không cần validate vì input comment đã được validate không được null\n",
        "                    dict_comment['Mã sản phẩm'] = product_id\n",
        "\n",
        "                    dict_comment['Tên khách hàng'] = content_comment[0]\n",
        "\n",
        "                    dict_comment['Ngày'] = content_comment[1]\n",
        "\n",
        "                    dict_comment['Nội dung đánh giá'] = content_comment[2]\n",
        "\n",
        "                    comments_of_product.append(dict_comment)\n",
        "            except:\n",
        "                continue\n",
        "        #try catch để tránh trường hợp không có nút next\n",
        "        try:       \n",
        "            next_button = driver.find_element(By.CSS_SELECTOR, 'a[onclick=\"prodComment.Page_change(\\'next\\')\"]')\n",
        "            next_button.click()\n",
        "        except StaleElementReferenceException:\n",
        "            break\n",
        "        except NoSuchElementException:\n",
        "            break\n",
        "    driver.quit()        \n",
        "    return comments_of_product"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trích xuất thông tin sản phẩm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def extract_product(product_link, category_id):\n",
        "\n",
        "    response = requests.get(product_link, impersonate='chrome')\n",
        "    \n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    product = {}\n",
        "\n",
        "    comments_of_product = []\n",
        "    product['Liên kết'] = product_link\n",
        "\n",
        "    product_id_tag = soup.find('td', class_ ='data_sku')\n",
        "    if product_id_tag:\n",
        "        product_id = product_id_tag.get_text()\n",
        "        product['Mã sản phẩm'] = product_id.strip()\n",
        "    else:\n",
        "        product['Mã sản phẩm'] = np.nan\n",
        "\n",
        "    title_tag = soup.find('h1')\n",
        "    product['Tên sản phẩm'] = title_tag.get_text().strip() if title_tag else np.nan\n",
        "\n",
        "    product['Mã danh mục'] = category_id\n",
        "\n",
        "    price_tag = soup.find('span', class_='price')\n",
        "    product['Giá'] = price_tag.get_text().split('\\xa0đ')[0].strip() if price_tag else np.nan\n",
        "\n",
        "    old_price_tag = soup.find('p', class_='old-price')\n",
        "    product['Giá Thị Trường'] = old_price_tag.find('span' , class_ = 'price').get_text().split('\\xa0đ')[0].strip() if old_price_tag else np.nan\n",
        "\n",
        "    sold_tag = soup.find('div', class_='product-view-qty-num')\n",
        "    product['Số sản phẩm đã bán'] = sold_tag.get_text().split(' ')[2].strip() if sold_tag else np.nan\n",
        "\n",
        "    nxb_tag = soup.find('td', class_='data_publisher')\n",
        "    product['Nhà xuất bản'] = nxb_tag.get_text().strip() if nxb_tag else np.nan\n",
        "\n",
        "    author_tag = soup.find('td', class_='data_author')\n",
        "    product['Tác giả'] = author_tag.get_text().strip() if author_tag else np.nan\n",
        "\n",
        "    qty_page_tag = soup.find('td', class_='data_qty_of_page')\n",
        "    product['Số trang'] = qty_page_tag.get_text().strip() if qty_page_tag else np.nan\n",
        "\n",
        "    # lấy đánh giá trung bình và số lượt đánh giá\n",
        "    elems_review = soup.find('div', class_='product-view-tab-content-rating-chart')\n",
        "    avg_review_rate = elems_review.text.split()[0]\n",
        "    count_review = elems_review.text.split()[1]\n",
        "        # slicing phần tử trong item\n",
        "    try:\n",
        "        avg_review_rate = float(avg_review_rate[:-2]) \n",
        "    except:\n",
        "        avg_review_rate = np.nan\n",
        "    try:\n",
        "        count_review = int(count_review[1:]) \n",
        "    except:\n",
        "            count_review = np.nan\n",
        "    product['Đánh Giá trung bình'] = avg_review_rate \n",
        "    product['Số lượt đánh giá'] = count_review\n",
        "    \n",
        "    if product['Số lượt đánh giá'] != np.nan and product['Số lượt đánh giá'] > 0 and product['Số lượt đánh giá'] < 12:    \n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"--disable-notifications\")\n",
        "        driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "        driver.get(product_link)\n",
        "        driver.refresh()\n",
        "\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "\n",
        "        sleep(randint(2, 5))\n",
        "\n",
        "        comments = driver.find_element(By.CSS_SELECTOR, '.comment_list').find_elements(By.TAG_NAME, 'li')\n",
        "        for comment in comments:\n",
        "                try:\n",
        "                \n",
        "                    content_comment = comment.text.split('\\n')\n",
        "                    \n",
        "                    dict_comment = {}\n",
        "\n",
        "                    dict_comment['Mã sản phẩm'] = product['Mã sản phẩm']\n",
        "\n",
        "                    dict_comment['Tên khách hàng'] = content_comment[0]\n",
        "\n",
        "                    dict_comment['Ngày'] = content_comment[1]\n",
        "\n",
        "                    dict_comment['Nội dung đánh giá'] = content_comment[2]\n",
        "\n",
        "                    comments_of_product.append(dict_comment)\n",
        "                except :\n",
        "                    continue\n",
        "                \n",
        "        driver.quit()\n",
        "        return product, comments_of_product\n",
        "    \n",
        "    elif product['Số lượt đánh giá'] > 12:\n",
        "        comments_of_product = crawl_comments_selenium(product_link, product['Mã sản phẩm'] )\n",
        "        return product, comments_of_product\n",
        "    \n",
        "    return product, False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "({'Liên kết': 'https://www.fahasa.com/bup-sen-xanh-tai-ban-2020.html?fhs_campaign=CATEGORY',\n",
              "  'Mã sản phẩm': '8935244826487',\n",
              "  'Tên sản phẩm': 'Búp Sen Xanh (Tái Bản 2020)',\n",
              "  'Mã danh mục': 14,\n",
              "  'Giá': '61.200',\n",
              "  'Giá Thị Trường': '72.000',\n",
              "  'Số sản phẩm đã bán': '10k+',\n",
              "  'Nhà xuất bản': 'NXB Kim Đồng',\n",
              "  'Tác giả': 'Sơn Tùng',\n",
              "  'Số trang': '364',\n",
              "  'Đánh Giá trung bình': 5.0,\n",
              "  'Số lượt đánh giá': 5},\n",
              " [{'Mã sản phẩm': '8935244826487',\n",
              "   'Tên khách hàng': 'PhanLee',\n",
              "   'Ngày': '03/08/2024',\n",
              "   'Nội dung đánh giá': 'Sách hay lắm mn ơi,đọc sẽ hiểu được thêm về tuổi thơ của Bác Hồ và những nguyên nhân gì tác động khiến Bác ra đi tìm đường cứu nước,tuyệt vời nhe????❤️tiểu thuyết của Sơn Tùng cuốn mà hay lắm'},\n",
              "  {'Mã sản phẩm': '8935244826487',\n",
              "   'Tên khách hàng': 'Ph******',\n",
              "   'Ngày': '19/09/2023',\n",
              "   'Nội dung đánh giá': 'sách hay lắm ạ. cả nhà nên mua nhé. Fahasa giao nhanh xỉu,djhg;kldiyghio/sgirgfydhfddjidddddddddddddddddddnjgkhfv hbgfihposdisdkghbg'},\n",
              "  {'Mã sản phẩm': '8935244826487',\n",
              "   'Tên khách hàng': 'Han Linh',\n",
              "   'Ngày': '10/08/2020',\n",
              "   'Nội dung đánh giá': 'Búp Sen Xanh - Sơn Tùng. Đây là cuốn sách về cậu bé Nguyễn Sinh Côn từ khi lớn lên cho tới lúc trưởng thành là chàng thanh niên Nguyễn Tất Thành. Những câu chuyện bên trong đem lại hai cảm giác rất rõ ràng như thế này: Sự cảm động và khâm phục bởi những suy nghĩ và hành động vì dân vì nước của các bậc nho sĩ, và sự ấm áp rất đỗi dịu êm bởi tình người. Khung cảnh mở ra tại làng Chùa, với đời sống người dân tuy giản dị mà đằm thắm, đầy sự quan tâm và xởi lởi. Cậu bé Côn được sinh ra ở đó, và trưởng thành trong sự bao dung chăm sóc của mẹ, của bà, của các chị các cô, cùng với sự dạy bảo nghiêm khắc của cha là anh nho Sắc. Thiên nhiên và tình người đã nuôi dưỡng ra một tâm hồn đẹp, ham học hỏi, ham suy nghĩ, mà cũng đầy đạo đức và lễ độ. Búp Sen Xanh không phải một câu chuyện đao to búa lớn hay cố gắng để nâng tầm một con người. Đó là một dòng chảy rất đỗi bình thường như bao tuổi thơ và quá trình lớn lên của nhiều đứa trẻ khác. Nguyễn Sinh Côn cũng lớn lên ở làng quê Việt Nam yên bình, có người lao động, có tiếng chó sủa, có tiếng hát của những cô gái dệt vải, có tiếng lũ trẻ con nô đùa, có tiếng những học trò học bài trên lớp, những lần bỏ học ra ngoài câu cá, những lần đi trêu chó nhà hàng xóm bị bắt được, những lời dạy của bà ngoại mượn lời ca dao, tục ngữ, hay hình ảnh các nho sĩ đàm đạo với nhau trong đêm sâu… Một xã hội Việt Nam đã cũ được ghi lại với đủ đầy những lễ nghi, những chuyển động cuộc sống. Điều khiến câu chuyện trở nên khác thường, thậm chí phi thường, ấy là từ những gì mà cậu bé Côn được dạy bảo từ khi còn tấm bé, là từ thiên tư phi thường của Côn, là từ căn nhà gia giáo nuôi dưỡng một con người mang chí anh hùng.'},\n",
              "  {'Mã sản phẩm': '8935244826487',\n",
              "   'Tên khách hàng': 'Minh Minh',\n",
              "   'Ngày': '03/08/2020',\n",
              "   'Nội dung đánh giá': 'Thật tuyệt vời. Lâu lắm rồi mình mới đọc cuốn sách hay và ý nghĩa đến vậy . Trước giờ mình thấy lịch sử học sao mà khô khan thế , nhưng khi tiếp cận với cuốn sách này thì mình thấy yêu và hiểu nhiều hơn về lịch sử . Cuốn sách này viết về Bác , từ thuở ấu thơ đến lúc ra đi tìm đường cứu nước tại bến cảng nhà Rồng . Tất cả đều hiện lên chân thực , sống động qua lời kể rất gần gũi của tác giả . Mình chứng kiến lại gia đình của Bác , nơi người con của Tổ quốc ấy được sinh ra như thế nào , được dạy dỗ rèn luyện ra sao , và sau này lớn lên thành người tài đức vẹn toàn bao người ngưỡng mộ . Điều khiến mình yêu và tự hào cũng như ngưỡng mộ nhất ở Bác chính là lòng thương người , yêu nước . Hi vọng nhiều bạn sẽ đọc được cuốn sách này , nó thực sự rất ý nghĩa . Sách của fahasa chưa bao giờ làm mình thất vọng . Lúc nào cũng đẹp,mới , giao hàng nhanh và có tâm .'},\n",
              "  {'Mã sản phẩm': '8935244826487',\n",
              "   'Tên khách hàng': 'Mai Mít',\n",
              "   'Ngày': '03/08/2020',\n",
              "   'Nội dung đánh giá': 'Đồng bào ai cũng \" kính \" công ơn cứu nước của Bác, nhưng để trọn vẹn hai chữ \" kính yêu \" thì phải hiểu được một khía cạnh nào đó của Người , và tác giả đã chọn kể cho ta về những ngày Người mới chào đời cho tới khi ra đi tìm đường cứu nước , một bản tóm tắt đầy cảm xúc quá trình trưởng thành của Người - \" Viết về tuổi thơ của các vĩ nhân là để con người biết đúng đường mà học tập vì ai cũng từng là trẻ con\". Sơn Tùng đáng được ngưỡng mộ vì những cố gắng vượt qua bệnh tật , khó khăn để thu thập những thước tư liệu quý giá về Bác từ những người đã từng trực tiếp ăn ở với Bác, gom thành những trang văn chân thật kể từ thuở hai cụ nội của Bác vừa nên duyên, những sự việc và con người góp phần nuôi dưỡng ý chí cách mạng của cậu bé Côn năm nào, và một đoạn ngắn về người giai nhân đầu đời có tên Út Huệ . Đoạn làm mình xúc động nhất là lúc Bác từ biệt cha để xuất dương, cụ Sắc đã bảo \" Phút giây này con đừng gọi cha, hãy gọi Tổ quốc , Đồng bào ! Con đi đi con \" , đây cũng là lần cuối cùng hai cha con gặp nhau .Một điểm ấn tượng nữa là vẻ đẹp tảo tần điển hình của người phụ nữ xưa, hết mình chăm lo cho chồng con của chị Sắc , và nó lại làm cho mình mâu thuẫn ở điểm tư tưởng cụ Sắc tiến bộ nhưng vẫn coi nam nhân chỉ việc học và luận chuyện lớn , vẫn để cho vợ mình gồng gánh tất cả việc nhà để rồi chết trẻ vì kiệt sức ư ? Hoàng Thị Loan đã sinh ra một người con vĩ đại nhưng cuộc đời lại quá tàn nhẫn với bà .Cuốn này không cho mình quá nhiều kiến thức lịch sử, nhưng nó mang trái tim của mình đến gần Bác hơn.'},\n",
              "  {'Mã sản phẩm': '8935244826487',\n",
              "   'Tên khách hàng': 'Phương Uyên',\n",
              "   'Ngày': '30/07/2020',\n",
              "   'Nội dung đánh giá': 'Cũng là một tín đồ cũng những cuốn sách kĩ năng bổ ích, những cuốn tiểu thuyết trong và ngoài nước thú vị,...Lúc đầu tôi đã nghĩ đến việc review những cuốn sách nổi tiếng như: Tuổi trẻ đáng giá bao nhiêu?; Nhà giả kim; Tony buổi sáng...và cả bộ truyện hay nhất thời đại của nhà văn J.K.Rowling-Harry Potter. Đó đều là những cuốn sách có tiếng vang lớn trong làng \" mọt sách\" và làm nên tên tuổi của chính tác giả bởi những giá trị thiết thực mà chúng đem lại cho người đọc. Sẽ là một thiếu sót nếu như bạn chưa tìm đọc đến chúng. Thế nhưng, ở đây tôi muốn chia sẻ cảm nhận về cuốn sách mà với tôi nó đã vượt lên trên tất cả những cuốn sách kia để đem đến cuộc thi giới thiệu cho các bạn cuốn \"Búp sen xanh\" của tác giả Sơn Tùng với mong muốn các bạn sẽ hiểu rõ hơn về một nhân vật quen thuộc mà tôi sẽ bật mí sau đây được tác giả xây dựng trong cuốn sách và cũng xin một ghi chú nhỏ đó là tác giả không phải người mà hầu hết các bạn đang nghĩ đến- một nghệ sĩ nào đó trong lĩnh vực âm nhạc đâu nhé^^! Tựa sách \" Búp sen xanh\" gợi cho bạn những gì? Có thể nhiều bạn đã biết đến rồi, có bạn chắc cũng chưa từng nghe qua. Nội dung cuốn sách tôi sẽ không kể rõ vì đây là câu chuyện viết về một nhân vật mà bất kì người Việt Nam nào cũng biết, một nhân vật gắn liền với độc lập dân tộc và chủ nghĩa xã hội Việt Nam: Hồ Chí Minh- chính là Bác Hồ kính yêu của chúng ta. Hồ Chí Minh trong tâm khảm của đa số mọi người bảo gồm tôi trước khi đọc cuốn sách là hình ảnh một vị lãnh tụ yêu nước thương dân, hy sinh cả cuộc đời để giải phóng dân tộc. Nhưng \"Búp sen xanh\" sẽ đem lại cho bạn một hình ảnh khác, hoàn toàn không đối lập mà là tiền đề quan trọng để chúng ta hiểu vì đâu và quá trình nào hình thành một nhân cách lớn như vậy. Nêu cao khẩu hiệu \" Học tập và làm theo tấm gương đạo đức Hồ Chí Minh\" vậy bạn biết gì về cậu bé tên Côn? Người thầy giáo Nguyễn Tất Thành? Nếu tò mò thì hãy tìm đọc cuốn sách ngày đi nhé! Bạn biết không, tôi đã không khỏi xúc động ngày từ những trang sách đầu tiên về những sự kiện, những biến cố của cuộc đời Bác- cậu bé Côn. Biến cố ấy tưởng chừng có thể đánh gục bất cứ đứa trẻ nào ở độ tuổi của Bác bấy giờ. Với cậu bé Côn, không chỉ bởi lòng thương cảm mà giọt nước mắt ấy còn là sự hổ thẹn, sự ngưỡng mộ của tôi đến với một ý chí kiên cường. Câu chuyện của cậu bé Côn thường rủ bạn bè trêu chó để chúng sủa om xòm đến tai quan Phó bảng, kết quả là cậu bé Côn phải chịu một hình phạt nghiêm khắc từ cha. Ông nói từng tiếng như búa đóng đinh: \" Từ ngày mai mỗi bữa còn ăn bớt một bát cơm, rảy sọt đi lặt phân bò, chiều về con viết bài từ \\'nhân cách\\' vào 50 trang giấy khổ rộng, mỗi trang tám hàng rồi nộp cho cha\". Đó chỉ là một trong số rất nhiều câu chuyện về tuổi đầu đời của Bác. Để chúng ta thấy được rằng: một vị lãnh tụ thiên tài, một danh nhân văn hoá không thể bỗng dưng suất hiện. Mọi sự vật, hiện tượng trong tự nhiên, xã hội đều có nguồn gốc. Đạo đức, tư tưởng và cả năng lực thiên tài của Bác cũng vậy. Cuốn sách \" Búp sen xanh\" sẽ cho ta thấy tuổi thơ của Bác được tắm mình trong môi trường giáo dục nghiêm khắc và mực thước ra sao để hình thành nhân cách cao đẹp sau này. Bác là người thật, việc thật, được sinh ra như bao người bình thường khác. Cuốn sách cũng đề cập đến những nhân vật lịch sử có thật như Phan Bội Châu, Đặng Thái Vân, Vương Thúc Quý,... Mỗi nhân vật đều mang một mét riêng nhưng tất cả tạo nên một bức tranh toàn cảnh về một lớp trí thức yêu nước cuối thế kỉ 20. Gấp lại cuốn sách, nó đã khiến tôi thấy đổi rất nhiều về cách suy nghĩ, cách giải quyết một vẫn đề, cân nhắc cẩn thận trước những lợi ích các nhân và lợi ích tập thể, dám chịu trách nhiệm cho việc làm của mình...Tất cả những điều đó đều học được từ tấm gương đạo đức Hồ Chí Minh. Học tập Bác không nhất thiết để phải trở thành một người vĩ đại. Mà lòng yêu mến, hiểu rõ và ngưỡng mộ một nhân cách cao đẹp sẽ khiến con người trở nên tốt đẹp hơn. Có thể nói \"Búp sen xanh\" là món quà thiêng liêng mà tác giả Sơn Tùng dành tặng cho Bác và là cuốn sách phù hợp với mọi lứa tuổi ở mọi môi trường từ gia đình, giáo dục, ngày cả tôn giáo thì nhân cách, đạo đức Hồ Chí Minh luôn là thước đo chuẩn mực cho những bài học về nhân cách con người. Có rất nhiều điều để nói về Bác, về cuốn sách này, bạn đọc hãy tìm đến và cùng cảm nhận nhé!'}])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extract_product('https://www.fahasa.com/bup-sen-xanh-tai-ban-2020.html?fhs_campaign=CATEGORY', 14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lấy tất cả sản phẩm trong danh mục con - child"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "u2p_eU-paCHG"
      },
      "outputs": [],
      "source": [
        "def crawl_all_products_in_category(category_parent_name ,category_child_name, category_link, category_id):\n",
        "\n",
        "    json_products = []\n",
        "\n",
        "    json_comments = []\n",
        "\n",
        "    total_products = 0\n",
        "    #crawl nhiều nhất 10 pages mỗi page 48 sản phẩm co từng danh mục\n",
        "    # page = 1\n",
        "\n",
        "    # while True:\n",
        "    # Set lại total cho progress bar ứng với số page crawl\n",
        "    with tqdm(total=1,\n",
        "        dynamic_ncols=True,\n",
        "        unit=\" trang\",  # Đơn vị cho mỗi bước\n",
        "        colour=\"blue\",\n",
        "        desc=\"Page: \",\n",
        "        position=1\n",
        "        ) as pbar:\n",
        "        try:\n",
        "            #test chỉ crawl 2 trang\n",
        "\n",
        "            # for page in range(1, 10):\n",
        "            for page in range(1, 2):\n",
        "                response = requests.get(f\"{category_link}{query_crawl}{page}\" , impersonate='chrome')\n",
        "\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "                products = soup.find('ul', class_='products-grid').find_all('li')\n",
        "\n",
        "                if len(products) == 0 :\n",
        "                    break\n",
        "                else:\n",
        "                    for i in range(0,len(products)):\n",
        "                        # nếu series book thì bỏ qua\n",
        "                        product_link = products[i].find('a').attrs['href']\n",
        "                        series_book = r'(seriesbook\\-index\\-series)'\n",
        "                        match = re.search(series_book, str(product_link))\n",
        "                        if not match: \n",
        "                            \n",
        "                            dict_product, comments_of_product = extract_product(product_link, category_id)\n",
        "\n",
        "                            json_products.append(dict_product)\n",
        "                        else : \n",
        "                            continue\n",
        "                        if comments_of_product != False:\n",
        "                            \n",
        "                            json_comments.append(comments_of_product)\n",
        "\n",
        "                        total_products+=1\n",
        "                pbar.update(1)\n",
        "        except:\n",
        "            # nhảy vào khi hết trang trong range hoặc vòng while bị ngắt do hết page\n",
        "            print(f'Đã crawl hết trang hiện có trên danh mục {category_parent_name} - {category_child_name}')   \n",
        "\n",
        "        # page += 1\n",
        "\n",
        "    json_comments = sum(json_comments, [])\n",
        "\n",
        "    return json_products, json_comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tiến hành chạy crawl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2 file csv chứa danh mục parent và child"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "H8kZ1XkU5Rv5",
        "outputId": "d434f8d2-96bf-41ea-dee7-f295ba838a04"
      },
      "outputs": [],
      "source": [
        "#Tiến hành gọi hàm crawl_all_categories_c1 và crawl_all_categories_c2 để lấy dữ liệu\n",
        "parent_categories = crawl_all_categories_c1()\n",
        "\n",
        "#drop index 5 vì không phải danh mục sách\n",
        "parent_categories = parent_categories.drop(index = 5).reset_index(drop = True)\n",
        "\n",
        "child_categories = []\n",
        "\n",
        "# Lấy các danh mục con của từng danh mục cha\n",
        "for parent_category , links in zip(parent_categories['Tên danh mục c1'], parent_categories['Liên kết']): \n",
        "    child_category = crawl_all_categories_c2(parent_category, links)\n",
        "    child_categories.append(child_category)\n",
        "\n",
        "# concat các frame chứa trong mảng thành 1 mảng hoàn chỉnh, tự động reset index bằng ignore_index\n",
        "child_categories = pd.concat(child_categories , ignore_index=True) \n",
        "\n",
        "# Xuất csv cho 2 frame chứa danh mục cấp parent và child\n",
        "parent_categories.to_csv('./data/raw/fahasha_parent_categories.csv', encoding='utf-8-sig', index=False)\n",
        "child_categories.to_csv('./data/raw/fahasha_child_categories.csv', encoding='utf-8-sig', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74a2949c620646e2a3ec97768e2e695a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Crawling:   0%|          | 0/1 [00:00<?, ? bước/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2627a8107a7d4ae4afd6ab9c7e229a71",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Page:   0%|          | 0/1 [00:00<?, ? trang/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đã lưu dữ liệu của danh mục Sách Trong Nước - Thiếu nhi vào thư mục data\\raw\\sach_trong_nuoc\\thieu_nhi\n"
          ]
        }
      ],
      "source": [
        "directory_paths = []\n",
        "# bỏ comment để chạy tất cả các danh mục con\n",
        "# crawl_categories = child_categories\n",
        "\n",
        "# ta sẽ chỉ crawl 2 danh mục sách trong dự án này tương đương 55 danh mục sp\n",
        "crawl_categories = child_categories[0:1]\n",
        "with tqdm(total=int(crawl_categories.shape[0]),\n",
        "        dynamic_ncols=True,\n",
        "        unit=\" bước\",  \n",
        "        colour=\"green\",\n",
        "        desc=\"Crawling: \",\n",
        "        position=0\n",
        "        ) as pbar:\n",
        "    for index in  range(0,int(crawl_categories.shape[0])) :        \n",
        "        #set tên danh mục c1 và c2 vào tiêu đề của progress bar\n",
        "        pbar.set_postfix({\"Danh Mục\": child_categories['Tên danh mục c1'].loc[index] + '-' + child_categories['Tên danh mục c2'].loc[index]})\n",
        "\n",
        "        category_c1_name = child_categories['Tên danh mục c1'].loc[index]\n",
        "        category_c2_name = child_categories['Tên danh mục c2'].loc[index]\n",
        "\n",
        "        category_link = child_categories['Liên kết'].loc[index]\n",
        "        category_id = child_categories['Mã danh mục'].loc[index]\n",
        "\n",
        "        json_products , json_comments = crawl_all_products_in_category(category_c1_name, category_c2_name, category_link, category_id)\n",
        "\n",
        "        df_products = pd.json_normalize(json_products)\n",
        "        df_comments = pd.json_normalize(json_comments)\n",
        "\n",
        "        directory_c1 = unidecode(category_c1_name.lower()).replace(' ','_')\n",
        "        directory_c2 = unidecode(category_c2_name.lower()).replace(' ','_')\n",
        "\n",
        "        directory_path = Path(f'{local_path}/{directory_c1}/{directory_c2}')\n",
        "\n",
        "        directory_path.mkdir(parents=True, exist_ok=True)\n",
        "        directory_paths.append(directory_path)\n",
        "\n",
        "        df_comments.to_csv(directory_path / 'comments.csv', encoding='utf-8-sig', index=False)\n",
        "        df_products.to_csv(directory_path / 'products.csv', encoding='utf-8-sig', index=False)\n",
        "        \n",
        "        pbar.write(f\"Đã lưu dữ liệu của danh mục {category_c1_name} - {category_c2_name} vào thư mục {directory_path}\")\n",
        "        pbar.update(1)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gộp các file csv của các danh mục"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Tạo mảng các path lưu trữ các file csv trong từng danh mục"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "directory_paths = []\n",
        "for index in  range(0,child_categories.shape[0]):\n",
        "        # print('Crawling category:', child_categories['Tên danh mục c2'].loc[index ])\n",
        "        category_c1_name = child_categories['Tên danh mục c1'].loc[index]\n",
        "        category_c2_name = child_categories['Tên danh mục c2'].loc[index]\n",
        "\n",
        "        directory_c1 = unidecode(category_c1_name.lower()).replace(' ','_')\n",
        "        directory_c2 = unidecode(category_c2_name.lower()).replace(' ','_')\n",
        "\n",
        "        directory_path = Path(f'{local_path}/{directory_c1}/{directory_c2}')\n",
        "\n",
        "        directory_path.mkdir(parents=True, exist_ok=True)\n",
        "        directory_paths.append(directory_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hàm gộp csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def merged_csv(object):\n",
        "\n",
        "    df_merged = pd.DataFrame()\n",
        "    for path in directory_paths:\n",
        "        #Do chưa crawl đầy đủ nên có thể có trường hợp không có file csv\n",
        "        #Bỏ qua các trường hợp không có file csv\n",
        "        try:\n",
        "            df = pd.read_csv(path/f'{object}.csv')\n",
        "            df_merged = pd.concat([df_merged, df], ignore_index=True)\n",
        "        except:\n",
        "            continue\n",
        "    return df_merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Tiến hành gộp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "products_merged = merged_csv('products')\n",
        "products_merged.to_csv(f'{local_path}/fahasha_products.csv', encoding='utf-8-sig', index=False)\n",
        "\n",
        "comments_merged = merged_csv('comments')\n",
        "comments_merged.to_csv(f'{local_path}/fahasha_comments.csv', encoding='utf-8-sig', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
